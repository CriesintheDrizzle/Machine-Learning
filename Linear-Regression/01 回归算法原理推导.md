# 一、线性回归

## 1、引入
**回归**
例子：机器学习在有监督学习算法中主要有两种类别：分类与回归

分类的概念比较简单，比如说一个人要去银行贷款，最后银行会根据种种因素来决定是否贷给这个人钱，有一个明确的几个类别的概念

回归，就不是银行是否贷给你钱了，而是银行会贷给你的概率是多少的问题了

## 2、明确目标
有如下数据，该数据中包含工资和年龄（2个特征）
其目标是预测银行会贷款给我多少钱（标签）
考虑：工资和年龄都会影响最终银行贷款的结果，那么它们各自有多大影响呢？（参数）

| 工资 | 年龄 | 额度 |
| --- | --- | --- |
| 4000 | 25 | 20000 |
| 8000 | 30 | 70000 |
| 5000 | 28 | 35000 |
| 7500 | 33 | 50000 |
| 12000 | 40 | 85000 |

工资：X<sub>1</sub>  年龄：X<sub>2</sub>  额度：Y——以上数据任务是一个有监督学习，有监督学习是指在训练一个模型，做一件事的时候必须得有一个标签或Y值作为指标

额度会受工资和年龄的影响，我们假设构建如下公式：
$$Y = X_{1} + X_{2}$$
这个式子可以反应额度受工资和年龄的影响，但是存在一个问题：X<sub>1</sub>和X<sub>2</sub>的系数相同，这样就代表着工资和年龄对于额度的影响是一样的，这很明显不符合常识。

现在，我们对上述式子进行变形：
$$Y = θ_{1}X_{1} + θ_{2}X_{2}$$
我们假设θ1 = 3，而θ2 = 1，这就说明工资的影响程度比年龄大3倍。

我们现在所需要做的就是，在已知条件下X1、X2、Y，确定最合适θ1与θ2，这样线性回归方程就求解出来了。

需要我们注意的是，我们在求解线性回归方程的时候，虽然不能满足所有的样本点，但是我们需要尽可能多的去满足样本点。
![[Pasted image 20230919223725.png]]
那么什么样的一个平面才是拟合最好的一个平面呢？使用什么指标来进行描述呢？见下。

## 3、误差项定义
假设θ<sub>1</sub>是年龄参数，θ<sub>2</sub>是工资参数
我们使用下面式子进行拟合平面：
$$h_{θ}(x) = θ_{1}X_{1} + θ_{2}X_{2} + θ_{0}$$
其中θ<sub>1</sub>与θ<sub>2</sub>我们前面说过是权重项，那这个θ<sub>0</sub>是什么呢？
这个θ<sub>0</sub>是偏置项，其起到一个微调模型的作用（使平面上下移动），无论θ<sub>1</sub>和θ<sub>2</sub>的大小是什么都不影响θ<sub>0</sub>的大小，但是核心还是在θ<sub>1</sub>和θ<sub>2</sub>上。

还有一个值得注意的点是：
当我们用上述式子进行拟合平面时会出现一个问题。我们使用代码对数据进行处理的时候是对一个<font color="#ff0000">矩阵</font>进行一系列处理，

| 样本 | 参数一 | 参数二 | 参数三 |
| --- | --- | --- |
| ① | ... | ... | ... |
| ② | ... | ... | ... |
| ③ | ... | ... | ... |
但是上述式子不能将其作为一个矩阵来处理，因为多了一个偏置项θ<sub>0</sub>，但我们通过下述变化就能解决这个问题，即对θ<sub>0</sub>处加上一个参数X<sub>0</sub>：
$$h_{θ}(x) = θ_{1}X_{1} + θ_{2}X_{2} + θ_{0}X_{0}$$
我们可以在数据预处理阶段对矩阵新增一列全1：

| 样本 | 参数一 | 参数二 | 参数三 | X1 |
| --- | --- | --- | --- | --- |
| ① | ... | ... | ... | 1 |
| ② | ... | ... | ... | 1 |
| ③ | ... | ... | ... | 1 |
这样就能保证偏置项的值不会改变，并且能对其进行矩阵运算

整合如下：
![[Pasted image 20230919223925.png]]

真实值和预测值之间肯定是要存在差异的（用yibuxiu来表示）
对于每个样本：
$$y^{(i)}=θ^{T}X^{(i)} + yibuxiu^{i}$$

希望训练出来的误差项越小越好，这就是判断平面拟合好坏的一个标准

## 4、独立同分布的意义
> 误差ɛ<sup>(i)</sup>是独立且具有相同的分布，并且具有相同的分步，并且服从均值为0方差为θ<sup>2</sup>的高斯分布

我们来对上面这句话进行解释：
1、独立：张三和李四一起来贷款，他俩没关系，样本与样本之间没有任何关系，这就是为什么需要对数据进行洗牌操作
2、同分布：我们假设他们两个都来的都是同一家银行，数据来源相同
3、高斯分布：银行可能会多给，也可能会少给，但是绝大多数情况下这个浮动不会太大，极小情况下浮动会比较大，符合正常情况，即所谓的正态分布
![[Pasted image 20230919224120.png]]

## 五、似然函数的作用
我们再来回顾一下预测值与误差的式子（1）：
$$y^{(i)}=θ^{T}x^{(i)}+yibuxiu^{(i)}$$
我们的注重点在如何求解θ上
又因为误差服从高斯分布（2）：
![[Pasted image 20230919224901.png]]
将（1）式代入（2）式：
![[Pasted image 20230919225020.png]]

见上式，我们想要找到一种θ与x的组合，并且与真实值越接近越好，即p越大越好。

**似然函数**
其公式如下：
![[Pasted image 20230919225806.png]]
公式的解释如下：
什么样的参数跟我们的数据组合后恰好是真实值。
上述式子使用的是累乘的符号，为什么是累乘呢？因为我们要考虑所有的数据来进行分析，那为什么是乘而不是加呢？因为我们的数据是独立同分布的，所以能使用连乘

但是乘法是非常难解的，我们需要把其变成加法：取对
但是对其进行取对操作之后我们最后求解的值会发生改变，那为什么我们能进行这个转换呢？这是因为我们有一个前提：我们并不关系L(θ)的值是多少，我们所求的是θ的值。即极值点是多少，取对并不会改变极值点。
这就变成了连加的操作：
**对数似然**
![[Pasted image 20230919231030.png]]
乘法难解，加法就容易了，对数里面乘法可以转换成加法，见上式

展开化简得如下式子：
![[Pasted image 20230919231240.png]]
我们希望上述式子越大越好，那么求需要减号后面的一项越小越好，因为减号前面的一部分是常数项

得到的目标函数如下，我们希望该目标函数越小越好：
![[Pasted image 20230919231738.png]]

## 六、参数求解
经过上述推导我们得到了一个目标函数如下
![[Pasted image 20230919232122.png]]
上式的x，y，θ是一个矩阵，不能当成一个数

现在我们需要求解的是，在什么θ情况下J(θ)最小，那么就需要求偏导，让偏导等于0：
![[Pasted image 20230919232430.png]]
令偏导等于0，最后求解的结果如下：
![[Pasted image 20230919232501.png]]

![[Pasted image 20230919235229.png]]
但是，上述过程并没有体现出机器学习

## 七、梯度下降
要体现出机器学习的方法，在我们得到一个目标函数后该如何求解呢？直接求解得到的值能作为最后的结果吗？我们发现其并不一定可解，线性回归中得到的最后的结果可以当作是一个特例

实现方法—梯度下降
机器学习的套路就是我交给机器一堆数据，然后告诉它什么样的学习方式是对的（目标函数），然后让它朝着这个方向去做
见下图：
![[Pasted image 20231212224943.png]]
![[a9c2c81aace274f8b4d720c89416538.jpg]]
![[e440eeed7eb281832d63cdb7805e6a8.jpg]]
![[f40968a1822d497e02119d40cfbfe9e.jpg]]

我们如何进行优化：
一口吃不成个胖子，我们要静悄悄的一步步的完成迭代（每次优化一点点）

## 八、参数更新的方法
当目标函数如下时：
![[Pasted image 20230919234725.png]]
其中我们要求解两个参数θ<sub>0</sub>和θ<sub>1</sub>
现在我们想一个问题，这两个参数是同时求解还是分开求解——分开求解
这两个参数没有关系，但是对最后的结果会产生影响

我们先对θ<sub>0</sub>进行求偏导得到一个方向，再对θ<sub>1</sub>进行求导得到一个方向，最后再将这两个向量进行求和

寻找山谷的最低点，也就是我们的目标函数终点（什么样的参数能使得目标函数达到极值点）

下山分几步走呢？（更新参数）
* 找到当前最合适的方向
* 走那么一小步，走快了会出现“跌倒”
* 按照方向与步伐去更新我们的参数
![[Pasted image 20230919235504.png]]

我们如何在数学上描述这么一个下山任务
我们使用下述式子来表示目标函数：
![[Pasted image 20230920000001.png]]
相较于之前的式子我们就多除了一个m，取平均
![[Pasted image 20230920000356.png]]
见上图，我们使用第一个式子求得了梯度的方向，再使用第二个式子表示梯度下降，即与梯度相反的方向，这就实现了参数的更新
但是这种方法就像上图所说的一样，这种方法很慢

![[Pasted image 20230920000900.png]]
随机梯度下降省去了求平均的操作，速度加快。缺点见上

![[Pasted image 20230920001117.png]]
综合上述两个方法，得到小批量梯度下降法，上述式子选择的batch是10，可以自由选择
batch越大越精确，但速度越慢，batch经常取2的倍数

**学习率**
学习率（步长）：对结果会产生影响，一般很小，即我们前面所说的前进一小点
如何选择：从小的时候，不行再小
批处理数量：32，64，128都可以，很多时候还得考虑内存和效率








